---
title: "IL_Decision_Tree"
output: html_notebook
author: "Omkar Pawar [Add your names here]"
---

# Decision Tree Algorithm for classification

### Import Data and essential libraries.

```{r}
library(sys)

working_directory <- getwd()

setwd(dirname(dirname(working_directory)))

# The models are saved in this directory.
model_path <- paste(dirname(rstudioapi::getActiveDocumentContext()$path), "models", sep = "/")
print(model_path)

library(corrplot)
library(ggplot2)
library(tidyr)
library(stringr)
library(dplyr)
library(data.table)
library(mice)
library(rstudioapi)    
library(naniar)
library(caret)
library(pROC)
library(ROCR)
library(lmtest)

source(paste(dirname(dirname(dirname(rstudioapi::getActiveDocumentContext()$path))), "utils/utils.r", sep="/"))

source(paste(dirname(dirname(dirname(rstudioapi::getActiveDocumentContext()$path))), "utils/model_utils.r", sep="/"))
```

```{r}
hmda_data_df_ <- fread("/Users/omkarpawar/Desktop/Data/imputed_il_2017.csv")
hmda_data_df <- as.data.frame(hmda_data_df_)
```

```{r}
colnames(hmda_data_df)
```
 
# Create dataframe based on selected column list from the source dataframe.
```{r}
df_for_model <- hmda_data_frame_for_model(hmda_data_df)
df_for_model <- process_model_df_columns(df_for_model)

```

## Split Data into training and testing sets using stratified sampling
```{r}
set.seed(400)

split_dfs <- train_test_split(df_for_model$loan_granted, df_for_model)

train <- split_dfs[[1]]
test <- split_dfs[[2]]
```
## CART Algorithm
Classification and Regression Trees (CART) split attributes based on values that minimize a loss function, such as sum of squared errors.

From the selected columns in EDA for feature importances, we select top features and build a model using those predictors. 
```{r}
library(rpart)
#Create Model
cart_model_basic <- rpart(loan_granted~lien_status+purchaser_type+agency_code+applicant_race_and_ethnicity+applicant_income_000s+county_code,train,control = rpart.control(cp = 0))

pred.model.1 <- predict(cart_model_basic,test,type = "class")
confusionMatrix(pred.model.1,test$loan_granted)
# Prune the model
pruned  <- prune(cart_model_basic , cp = 0.0084)
pred <- predict(pruned,test,type = "class" )
confusionMatrix(pred,test$loan_granted)
precision_recall(confusionMatrix(pred,test$loan_granted)$table)
```
```{r}
library(rpart.plot)
rpart.plot(pruned, extra = 106)
```

## Now we try a different tree algorithm. The C4.5 algorithm is an extension of the ID3 algorithm and constructs a decision tree to maximize information gain (difference in entropy).

```{r}
library(RWeka)

c4.5_model <- J48(loan_granted~lien_status+purchaser_type+agency_code+loan_to_income_ratio+applicant_race_and_ethnicity+applicant_income_000s+county_code,train)

pred <- predict(c4.5_model,test,type = "class")
confusionMatrix(pred,test$loan_granted)
precision_recall(confusionMatrix(pred,test$loan_granted)$table)
```
## Calculate Predicted Probabilites
```{r}
predicted.probabilities.cart <- predict(cart_model_basic,test,type = "prob")
predicted.probabilities.c45 <- predict(c4.5_model,test,type = "prob")
```
## Print Model Summary and plots for both models
### Cart Model
```{r}
model_performance_curves(predicted.probabilities.cart[,2],test$loan_granted)
print_model_performance(predicted.probabilities.cart[, 2], test, positive = "yes", negative = "no", threshold = 0.6)
```
### C4.5 Model
```{r}
model_performance_curves(predicted.probabilities.c45[,2],test$loan_granted)
print_model_performance(predicted.probabilities.c45[, 2], test, positive = "yes", negative = "no", threshold = 0.6)
```

